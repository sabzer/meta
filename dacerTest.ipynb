{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4454f556",
   "metadata": {},
   "source": [
    "# DACER\n",
    "(trying to use DACER with metaworld, but it was originally used with gymnasium)\n",
    "(wrote many suspicious wrappers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c250df",
   "metadata": {},
   "source": [
    "# The following appears to work!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ccb03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saba/miniconda3/envs/relax/lib/python3.11/site-packages/gymnasium/spaces/box.py:235: UserWarning: \u001b[33mWARN: Box low's precision lowered by casting to float32, current low.dtype=float64\u001b[0m\n",
      "  gym.logger.warn(\n",
      "/home/saba/miniconda3/envs/relax/lib/python3.11/site-packages/gymnasium/spaces/box.py:305: UserWarning: \u001b[33mWARN: Box high's precision lowered by casting to float32, current high.dtype=float64\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistributionalQNet2(hidden_sizes=[256, 256, 256], activation=<function mish at 0x7fa122fef060>, output_activation=<function <lambda> at 0x7fa87ca11ee0>, name='distributional_q_net2')\n",
      "DistributionalQNet2(hidden_sizes=[256, 256, 256], activation=<function mish at 0x7fa122fef060>, output_activation=<function <lambda> at 0x7fa87ca11ee0>, name='distributional_q_net2')\n",
      "DACERPolicyNet(hidden_sizes=[256, 256, 256], activation=<function mish at 0x7fa122fef060>, output_activation=<function <lambda> at 0x7fa87ca13f60>, time_dim=16, name='dacer_policy_net')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'+ptx89' is not a recognized feature for this target (ignoring feature)\n",
      "'+ptx89' is not a recognized feature for this target (ignoring feature)\n",
      "'+ptx89' is not a recognized feature for this target (ignoring feature)\n",
      "'+ptx89' is not a recognized feature for this target (ignoring feature)\n",
      "'+ptx89' is not a recognized feature for this target (ignoring feature)\n",
      "'+ptx89' is not a recognized feature for this target (ignoring feature)\n",
      "'+ptx89' is not a recognized feature for this target (ignoring feature)\n",
      "'+ptx89' is not a recognized feature for this target (ignoring feature)\n",
      "'+ptx89' is not a recognized feature for this target (ignoring feature)\n",
      "'+ptx89' is not a recognized feature for this target (ignoring feature)\n",
      "'+ptx89' is not a recognized feature for this target (ignoring feature)\n",
      "'+ptx89' is not a recognized feature for this target (ignoring feature)\n",
      "'+ptx89' is not a recognized feature for this target (ignoring feature)\n",
      "'+ptx89' is not a recognized feature for this target (ignoring feature)\n",
      "'+ptx89' is not a recognized feature for this target (ignoring feature)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from types import SimpleNamespace\n",
    "from pathlib import Path\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from metaworld import MT10\n",
    "from gymnasium import ObservationWrapper, spaces\n",
    "from gymnasium.core import Wrapper\n",
    "import random\n",
    "import subprocess\n",
    "from relax.algorithm.dacer import DACER\n",
    "from relax.buffer import TreeBuffer\n",
    "from relax.network.dacer import create_dacer_net\n",
    "from relax.trainer.off_policy import OffPolicyTrainer\n",
    "from relax.utils.experience import Experience\n",
    "from relax.utils.fs import PROJECT_ROOT\n",
    "from relax.utils.random_utils import seeding\n",
    "\n",
    "#probably can pick better hyperparameters...\n",
    "seed=100\n",
    "hidden_num= 3\n",
    "hidden_dim =256\n",
    "diffusion_steps= 20\n",
    "diffusion_hidden_dim   = 256\n",
    "start_step =int(1e3)  \n",
    "total_step =int(1e6)\n",
    "lr=1e-4\n",
    "\n",
    "\n",
    "class OneHotTaskWrapper(ObservationWrapper):\n",
    "    def __init__(self, env, env_idx, all_tasks, n_envs):\n",
    "        super().__init__(env)\n",
    "        self.env_idx= env_idx\n",
    "        self.all_tasks= all_tasks\n",
    "        self.n_envs=n_envs\n",
    "\n",
    "        self.n_configs= len(all_tasks) // n_envs\n",
    "\n",
    "        orig_low,orig_high=env.observation_space.low,env.observation_space.high\n",
    "        low=np.concatenate([orig_low,np.zeros(n_envs,dtype=np.float32)])\n",
    "        high=np.concatenate([orig_high,np.ones(n_envs,dtype=np.float32)])\n",
    "        self.observation_space=spaces.Box(low=low,high=high,dtype=np.float32)\n",
    "\n",
    "#sample a task and config within task upon each env restart. task is random\n",
    "#can also restrict range of new tasks (<n for MTn) if you'd like\n",
    "    def reset(self, **kwargs):\n",
    "        cfg_idx=random.randrange(self.n_configs)\n",
    "        task=self.all_tasks[self.env_idx*self.n_configs+cfg_idx]\n",
    "        self.env.unwrapped.set_task(task)\n",
    "        obs,info=self.env.reset(**kwargs)\n",
    "        return self._append_one_hot(obs),info\n",
    "\n",
    "    def observation(self,obs):\n",
    "        return self._append_one_hot(obs)\n",
    "\n",
    "    def _append_one_hot(self, obs):\n",
    "        one_hot = np.zeros(self.n_envs,dtype=np.float32)\n",
    "        one_hot[self.env_idx]=1.0\n",
    "        return np.concatenate([obs, one_hot],axis=-1)\n",
    "\n",
    "# trying to bypass errors w/ more wrappers (chat suggested) (honestly kind of suspicious, not sure what internal logic is being bypassed)\n",
    "class SpecWrapper(Wrapper):\n",
    "    def __init__(self, env, env_id: str):\n",
    "        super().__init__(env)\n",
    "        self._spec = SimpleNamespace(id=env_id)\n",
    "    @property\n",
    "    def spec(self):\n",
    "        return self._spec\n",
    "\n",
    "ml10= MT10()\n",
    "all_tasks = ml10.train_tasks\n",
    "n_envs= len(ml10.train_classes)\n",
    "\n",
    "raw=ml10.train_classes[all_tasks[0].env_name]()\n",
    "raw.seed(seed)\n",
    "env=OneHotTaskWrapper(raw, all_tasks, n_envs)\n",
    "env=SpecWrapper(env, \"MT10\")\n",
    "\n",
    "#getting a lot of dimensionality issues\n",
    "obs_dim=env.observation_space.shape[0]\n",
    "act_dim=env.action_space.shape[0]\n",
    "eval_env=None\n",
    "\n",
    "\n",
    "master_rng, _ = seeding(seed)\n",
    "env_seed,env_action_seed,buffer_seed, init_key_seed,train_key_seed = map(\n",
    "    int,master_rng.integers(0, 2**32 - 1, 5)\n",
    ")\n",
    "init_key =jax.random.key(init_key_seed)\n",
    "train_key=jax.random.key(train_key_seed)\n",
    "\n",
    "buffer=TreeBuffer.from_experience(obs_dim, act_dim, size=int(1e6), seed=buffer_seed)\n",
    "\n",
    "def mish(x: jnp.ndarray)->jnp.ndarray:\n",
    "    return x*jnp.tanh(jax.nn.softplus(x))\n",
    "\n",
    "hidden_sizes=[hidden_dim] * hidden_num\n",
    "diffusion_sizes=[diffusion_hidden_dim] * hidden_num\n",
    "agent,params=create_dacer_net(\n",
    "    init_key,\n",
    "    obs_dim,\n",
    "    act_dim,\n",
    "    hidden_sizes,\n",
    "    diffusion_sizes,\n",
    "    mish,\n",
    "    num_timesteps=diffusion_steps\n",
    ")\n",
    "algorithm=DACER(agent, params, lr=lr)\n",
    "\n",
    "#using instead of OffPolicyTrainer bc it keeps giving eval error\n",
    "#tbh needs a whole rewrite\n",
    "class NoEvalTrainer(OffPolicyTrainer):\n",
    "    def setup(self, dummy_data):\n",
    "        Path(self.log_path).mkdir(parents=True, exist_ok=True)\n",
    "        orig_popen = subprocess.Popen\n",
    "        subprocess.Popen=lambda *args, **kwargs: SimpleNamespace(\n",
    "            stdin=SimpleNamespace(close=lambda: None),\n",
    "            wait=lambda *args, **kwargs: None\n",
    "        )\n",
    "        try:\n",
    "            super().setup(dummy_data)\n",
    "        finally:\n",
    "            subprocess.Popen = orig_popen\n",
    "\n",
    "log_dir=PROJECT_ROOT/\"logs\"/f\"MT10_dacer_{time.strftime('%Y-%m-%d_%H-%M-%S')}_s{seed}\"\n",
    "\n",
    "#now actually running DACER\n",
    "trainer=NoEvalTrainer(\n",
    "    env=env,\n",
    "    algorithm=algorithm,\n",
    "    buffer=buffer,\n",
    "    start_step=start_step,\n",
    "    total_step=total_step,\n",
    "    sample_per_iteration=1,\n",
    "    evaluate_env=eval_env,\n",
    "    save_policy_every=300_000,\n",
    "    warmup_with=\"random\",\n",
    "    log_path=log_dir,\n",
    ")\n",
    "trainer.setup(Experience.create_example(obs_dim, act_dim, trainer.batch_size))\n",
    "trainer.run(train_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71de330a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# semantics!\n",
    "contextArray = np.array([\n",
    "    [ 2.55721937e+00,  2.31504669e+00,  1.53086022e+00,  3.67148980e+00,\n",
    "     -7.59686234e-01, -7.61520546e-01,  7.11087296e-01, -2.76179305e-01,\n",
    "     -9.09933731e-03, -4.92187850e-16],\n",
    "    [ 3.19798408e+00,  2.29742495e-01,  1.50240787e+00, -7.02709710e-01,\n",
    "     -1.11196737e-01,  2.74924940e+00, -1.53620950e+00, -1.18464051e-01,\n",
    "      3.12674877e-02, -4.92187850e-16],\n",
    "    [ 3.28570139e+00,  1.92692870e+00,  2.16563078e-01, -3.17126572e+00,\n",
    "      1.22878090e+00, -1.05485421e+00,  1.25747231e+00,  3.66652176e-01,\n",
    "      2.40985359e-02, -4.92187850e-16],\n",
    "    [-2.08496350e+00,  4.64285575e-01,  5.41231677e-01,  6.38912049e-01,\n",
    "      2.08218306e+00, -1.46809404e+00, -2.13341308e+00,  7.39046881e-01,\n",
    "     -2.00950098e-01, -4.92187850e-16],\n",
    "    [ 4.27420596e-01, -8.70185256e-01, -2.86820046e+00,  3.71623910e-01,\n",
    "      1.72775859e+00, -2.57396544e-02, -3.41897534e-02, -1.64198974e+00,\n",
    "     -1.41869863e-01, -4.92187850e-16],\n",
    "    [ 1.15915385e-01, -1.24388372e+00, -2.73867371e+00,  1.45164344e+00,\n",
    "      5.18528498e-01,  1.21674038e+00,  9.57762804e-01,  1.37271833e+00,\n",
    "      3.60078564e-01, -4.92187850e-16],\n",
    "    [ 1.14941507e+00, -4.93749060e+00,  2.04079053e+00, -2.26013967e-01,\n",
    "     -7.33097844e-01, -1.07109692e+00,  3.29521154e-01, -4.60878162e-02,\n",
    "     -9.44210158e-03, -4.92187850e-16],\n",
    "    [-1.88857968e-01,  8.08267667e-01, -2.38279611e+00, -1.02331244e+00,\n",
    "     -3.41088830e+00, -9.48025784e-01, -1.05931369e+00,  2.31463462e-02,\n",
    "      9.69085706e-02, -4.92187850e-16],\n",
    "    [-4.71406517e+00,  8.02744073e-01,  1.52039680e+00, -5.83708654e-01,\n",
    "      3.05733353e-02,  4.24301726e-01,  5.31541815e-01, -4.67244928e-01,\n",
    "      1.12817178e+00, -4.92187850e-16],\n",
    "    [-3.74576926e+00,  5.04544376e-01,  6.37420105e-01, -4.26658712e-01,\n",
    "     -5.72955276e-01,  9.39039647e-01,  9.75740641e-01,  4.84021130e-02,\n",
    "     -1.27916354e+00, -4.92187850e-16]\n",
    "], dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d853e09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from types import SimpleNamespace\n",
    "from pathlib import Path\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from metaworld import MT10\n",
    "from gymnasium import ObservationWrapper, spaces\n",
    "from gymnasium.core import Wrapper\n",
    "import random\n",
    "import subprocess\n",
    "from relax.algorithm.dacer import DACER\n",
    "from relax.buffer import TreeBuffer\n",
    "from relax.network.dacer import create_dacer_net\n",
    "from relax.trainer.off_policy import OffPolicyTrainer\n",
    "from relax.utils.experience import Experience\n",
    "from relax.utils.fs import PROJECT_ROOT\n",
    "from relax.utils.random_utils import seeding\n",
    "\n",
    "#probably can pick better hyperparameters...\n",
    "seed=100\n",
    "hidden_num= 3\n",
    "hidden_dim =256\n",
    "diffusion_steps= 20\n",
    "diffusion_hidden_dim   = 256\n",
    "start_step =int(1e3)  \n",
    "total_step =int(1e6)\n",
    "lr=1e-4\n",
    "\n",
    "\n",
    "class OneHotTaskWrapper(ObservationWrapper):\n",
    "    def __init__(self, env, env_idx, all_tasks, n_envs):\n",
    "        super().__init__(env)\n",
    "        self.env_idx= env_idx\n",
    "        self.all_tasks= all_tasks\n",
    "        self.n_envs=n_envs\n",
    "\n",
    "        self.n_configs= len(all_tasks) // n_envs\n",
    "\n",
    "        orig_low,orig_high=env.observation_space.low,env.observation_space.high\n",
    "        low=np.concatenate([orig_low,np.zeros(n_envs,dtype=np.float32)])\n",
    "        high=np.concatenate([orig_high,np.ones(n_envs,dtype=np.float32)])\n",
    "        self.observation_space=spaces.Box(low=low,high=high,dtype=np.float32)\n",
    "\n",
    "#sample a task and config within task upon each env restart. task is random\n",
    "#can also restrict range of new tasks (<n for MTn) if you'd like\n",
    "    def reset(self, **kwargs):\n",
    "        cfg_idx=random.randrange(self.n_configs)\n",
    "        task=self.all_tasks[self.env_idx*self.n_configs+cfg_idx]\n",
    "        self.env.unwrapped.set_task(task)\n",
    "        obs,info=self.env.reset(**kwargs)\n",
    "        return self._append_one_hot(obs),info\n",
    "\n",
    "    def observation(self,obs):\n",
    "        return self._append_one_hot(obs)\n",
    "\n",
    "    def _append_one_hot(self, obs):\n",
    "        one_hot = np.zeros(self.n_envs, dtype=np.float32)\n",
    "        one_hot[self.env_idx] = 1.0\n",
    "        #to get the semantic embedding from semantic matrix + onehot\n",
    "        #use jax here in the future\n",
    "        one_hot_mod=np.einsum('ik,kj->ij', np.array([one_hot]),contextArray)[0]\n",
    "        return np.concatenate([obs, one_hot_mod], axis=-1)\n",
    "\n",
    "# trying to bypass errors w/ more wrappers (chat suggested) (honestly kind of suspicious, not sure what internal logic is being bypassed)\n",
    "class SpecWrapper(Wrapper):\n",
    "    def __init__(self, env, env_id: str):\n",
    "        super().__init__(env)\n",
    "        self._spec = SimpleNamespace(id=env_id)\n",
    "    @property\n",
    "    def spec(self):\n",
    "        return self._spec\n",
    "\n",
    "ml10= MT10()\n",
    "all_tasks = ml10.train_tasks\n",
    "n_envs= len(ml10.train_classes)\n",
    "\n",
    "raw=ml10.train_classes[all_tasks[0].env_name]()\n",
    "raw.seed(seed)\n",
    "env=OneHotTaskWrapper(raw, all_tasks, n_envs)\n",
    "env=SpecWrapper(env, \"MT10\")\n",
    "\n",
    "#getting a lot of dimensionality issues\n",
    "obs_dim=env.observation_space.shape[0]\n",
    "act_dim=env.action_space.shape[0]\n",
    "eval_env=None\n",
    "\n",
    "\n",
    "master_rng, _ = seeding(seed)\n",
    "env_seed,env_action_seed,buffer_seed, init_key_seed,train_key_seed = map(\n",
    "    int,master_rng.integers(0, 2**32 - 1, 5)\n",
    ")\n",
    "init_key =jax.random.key(init_key_seed)\n",
    "train_key=jax.random.key(train_key_seed)\n",
    "\n",
    "buffer=TreeBuffer.from_experience(obs_dim, act_dim, size=int(1e6), seed=buffer_seed)\n",
    "\n",
    "def mish(x: jnp.ndarray)->jnp.ndarray:\n",
    "    return x*jnp.tanh(jax.nn.softplus(x))\n",
    "\n",
    "hidden_sizes=[hidden_dim] * hidden_num\n",
    "diffusion_sizes=[diffusion_hidden_dim] * hidden_num\n",
    "agent,params=create_dacer_net(\n",
    "    init_key,\n",
    "    obs_dim,\n",
    "    act_dim,\n",
    "    hidden_sizes,\n",
    "    diffusion_sizes,\n",
    "    mish,\n",
    "    num_timesteps=diffusion_steps\n",
    ")\n",
    "algorithm=DACER(agent, params, lr=lr)\n",
    "\n",
    "#using instead of OffPolicyTrainer bc it keeps giving eval error\n",
    "#tbh needs a whole rewrite\n",
    "class NoEvalTrainer(OffPolicyTrainer):\n",
    "    def setup(self, dummy_data):\n",
    "        Path(self.log_path).mkdir(parents=True, exist_ok=True)\n",
    "        orig_popen = subprocess.Popen\n",
    "        subprocess.Popen=lambda *args, **kwargs: SimpleNamespace(\n",
    "            stdin=SimpleNamespace(close=lambda: None),\n",
    "            wait=lambda *args, **kwargs: None\n",
    "        )\n",
    "        try:\n",
    "            super().setup(dummy_data)\n",
    "        finally:\n",
    "            subprocess.Popen = orig_popen\n",
    "\n",
    "log_dir=PROJECT_ROOT/\"logs\"/f\"MT10_dacer_{time.strftime('%Y-%m-%d_%H-%M-%S')}_s{seed}\"\n",
    "\n",
    "#now actually running DACER\n",
    "trainer=NoEvalTrainer(\n",
    "    env=env,\n",
    "    algorithm=algorithm,\n",
    "    buffer=buffer,\n",
    "    start_step=start_step,\n",
    "    total_step=total_step,\n",
    "    sample_per_iteration=1,\n",
    "    evaluate_env=eval_env,\n",
    "    save_policy_every=300_000,\n",
    "    warmup_with=\"random\",\n",
    "    log_path=log_dir,\n",
    ")\n",
    "trainer.setup(Experience.create_example(obs_dim, act_dim, trainer.batch_size))\n",
    "trainer.run(train_key)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "relax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
